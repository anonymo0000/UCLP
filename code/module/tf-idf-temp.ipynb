{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None\n",
    "        self.classes = None\n",
    "        self.vocabulary = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
    "        num_classes = len(self.classes)\n",
    "        num_documents = len(y)\n",
    "\n",
    "        #          \n",
    "        self.class_probs = class_counts / num_documents\n",
    "\n",
    "        #      \n",
    "        self.vocabulary = set(word for doc in X for word in doc.split())\n",
    "\n",
    "        #          \n",
    "        self.word_probs = {word: np.zeros(num_classes) for word in self.vocabulary}\n",
    "\n",
    "        #              \n",
    "        for i, cls in enumerate(self.classes):\n",
    "            #           \n",
    "            class_docs = X[y == cls]\n",
    "            #           \n",
    "            for word in self.vocabulary:\n",
    "                #                   \n",
    "                word_count = sum(doc.split().count(word) for doc in class_docs)\n",
    "                #       \n",
    "                self.word_probs[word][i] = (word_count + 1) / (len(class_docs) + len(self.vocabulary))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for doc in X:\n",
    "            #      \n",
    "            probs = np.log(self.class_probs)\n",
    "            for word in doc.split():\n",
    "                if word in self.vocabulary:\n",
    "                    #              \n",
    "                    probs += np.log(self.word_probs[word])\n",
    "            #                \n",
    "            predicted_class = self.classes[np.argmax(probs)]\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "\n",
    "#     \n",
    "X_train = [\"  1\", \"  2\", \"  3\", \"  4\"]\n",
    "y_train = [\"  1\", \"  2\", \"  1\", \"  2\"]\n",
    "X_test = [\"  5\", \"  6\"]\n",
    "\n",
    "#              \n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#     \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#       \n",
    "print(\"    :\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m    “/usr/bin/python3”      ipykernel 。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_word_definition(word):\n",
    "    #              API     \n",
    "    app_key = 'YOUR_APP_KEY'\n",
    "    app_secret = 'YOUR_APP_SECRET'\n",
    "\n",
    "    #        API\n",
    "    url = 'https://openapi.youdao.com/api'\n",
    "    params = {\n",
    "        'q': word,\n",
    "        'from': 'auto',\n",
    "        'to': 'auto',\n",
    "        'appKey': app_key,\n",
    "        'salt': 'random_salt',\n",
    "        'sign': '',  #             ，     \n",
    "    }\n",
    "\n",
    "    #     \n",
    "    sign_str = app_key + word + 'random_salt' + app_secret\n",
    "    params['sign'] = hashlib.md5(sign_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "    #     \n",
    "    response = requests.get(url, params=params)\n",
    "    result = response.json()\n",
    "\n",
    "    #     \n",
    "    if result['errorCode'] == '0':\n",
    "        if 'basic' in result:\n",
    "            for explanation in result['basic']['explains']:\n",
    "                print(explanation)\n",
    "        elif 'translation' in result:\n",
    "            for translation in result['translation']:\n",
    "                print(translation)\n",
    "    else:\n",
    "        print('    ，    ：', result['errorCode'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word = \"sequences\"\n",
    "    get_word_definition(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#          ，            \n",
    "documents = [\"Buffer in overflows in PL/SQL module \",\"in the conteudo parameter.\"]\n",
    "labels = [\"1\"]\n",
    "\n",
    "# 1.      \n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "#           \n",
    "documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# 2.   TF-IDF      \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "# \n",
    "print(X)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "word2tfidf = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0]))\n",
    "# print(word2tfidf)\n",
    "#  ,   TF-IDF       \n",
    "sorted_word2tfidf = sorted(word2tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_word2tfidf)\n",
    "\n",
    "word2tfidf = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[1]))\n",
    "# print(word2tfidf)\n",
    "#  ,   TF-IDF       \n",
    "sorted_word2tfidf = sorted(word2tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_word2tfidf)\n",
    "\n",
    "# 3.      \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4.          \n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 5.   \n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6.     \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#           \n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m    “/usr/bin/python3”      ipykernel 。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#          ，            \n",
    "documents = [\"Buffer buffer in overflows in PL/SQL module \",\"in the conteudo parameter.\"]\n",
    "# labels = [\"  1\", \"  2\", ..., \"  N\"]\n",
    "\n",
    "# 1.      \n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "#           \n",
    "documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# 2.   TF-IDF      \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3.  TF-IDF       \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "#                TF-IDF  \n",
    "tfidf_values = X.toarray()\n",
    "\n",
    "#       ，         TF-IDF       \n",
    "sorted_indices = np.argsort(-tfidf_values, axis=1)\n",
    "\n",
    "#              TF-IDF       \n",
    "for i, indices in enumerate(sorted_indices):\n",
    "    doc = documents[i]\n",
    "    print(f\"  {i + 1}       TF-IDF   ：\")\n",
    "    for j in indices:\n",
    "        word = feature_names[j]\n",
    "        tfidf = tfidf_values[i, j]\n",
    "        print(f\"{word}: {tfidf}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # 3.      \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 4.          \n",
    "# naive_bayes_classifier = MultinomialNB()\n",
    "# naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # 5.   \n",
    "# y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "# # 6.     \n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# #           \n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
