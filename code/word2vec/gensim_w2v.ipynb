{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#     \n",
    "corpus = []\n",
    "with open(\"/home/user/Programs/PMA_lihang/data/allitems_v5.csv\", 'r',encoding='utf-8')as f: #     \n",
    "    lines = []\n",
    "    i = 0\n",
    "    for line in f: #       \n",
    "        #  line                \n",
    "        #line = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \"\", line)\n",
    "        #line = line.strip()\n",
    "\n",
    "        temp = line.split(\"##=divide=##\\t\")[1]  \n",
    "        temp = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", temp)\n",
    "        #          \n",
    "        temp = temp.strip()\n",
    "        temp = temp.split(\" \")\n",
    "        #      \n",
    "        temp = list(filter(None, temp))\n",
    "        #            \n",
    "        temp = [word.lower() for word in temp]\n",
    "        corpus.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # i+=1\n",
    "        # if i == 5:\n",
    "#         #     break\n",
    "# with open(\"/home/user/Programs/PMA_lihang/data/SecurityFocus3.xlsx\", 'r',encoding='utf-8')as f: #     \n",
    "#     lines = []\n",
    "#     i = 0\n",
    "#     for line in f: #       \n",
    "#         #  line                \n",
    "#         #line = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \"\", line)\n",
    "#         #line = line.strip()\n",
    "\n",
    "#         temp = line.split(\"##=divide=##\\t\")[1]  \n",
    "#         temp = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", temp)\n",
    "#         temp = temp.split(\" \")\n",
    "#         #      \n",
    "#         temp = list(filter(None, temp))\n",
    "#         #            \n",
    "#         temp = [word.lower() for word in temp]\n",
    "#         corpus.append(temp)\n",
    "#         # i+=1\n",
    "#         # if i == 5:\n",
    "#         #     break\n",
    "# print(corpus[0:5])#   5     \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/home/user/Programs/PMA_lihang/data/SecurityFocus3.xlsx\"\n",
    "sheetName='description'\n",
    "df = pd.read_excel(path, sheet_name=sheetName)\n",
    "data=df.values\n",
    "#print(\"       :\\n{}\".format(data))\n",
    "data1 = data.tolist()\n",
    "\n",
    "for i in range(len(data1)):\n",
    "\n",
    "    content = data1[i][0]\n",
    "    #print(content)\n",
    "    #  content                \n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    corpus.append(content)\n",
    "    # print(content)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('overflow', 0.6677830219268799),\n",
       " ('integer', 0.5718554258346558),\n",
       " ('xpcd', 0.5091075301170349),\n",
       " ('xferwan', 0.505237340927124),\n",
       " ('underflows', 0.5002908110618591),\n",
       " ('heap', 0.49776899814605713),\n",
       " ('overflows', 0.49698445200920105),\n",
       " ('cachefsd', 0.4951731264591217),\n",
       " ('getparentcontrolinfo', 0.49316853284835815),\n",
       " ('umn', 0.4888419806957245),\n",
       " ('20161012', 0.4875584840774536),\n",
       " ('fontforge', 0.48547688126564026),\n",
       " ('tftpserver', 0.4852831959724426),\n",
       " ('formsetsystoolddns', 0.48461851477622986),\n",
       " ('xtokkaetama', 0.4845421016216278),\n",
       " ('shannon', 0.48440077900886536),\n",
       " ('ccmplayer', 0.48045605421066284),\n",
       " ('formsetmacfiltercfg', 0.47998306155204773),\n",
       " ('pubconv', 0.4797396957874298),\n",
       " ('underwrite', 0.4791622757911682)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#   Word2Vec     ：size:      ；window:       ，min_count               \n",
    "model = Word2Vec(corpus,vector_size = 300, window = 3 , min_count = 3, workers=128, epochs=10, negative=10,sg=1,max_final_vocab = 50000)\n",
    "# print(\"      ：\\n\",model.wv.get_vector('  '))\n",
    "# print(\"\\n          20   ：\")\n",
    "model.save(\"word2vec.model\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.wv.most_similar('buffer', topn = 20)#         20   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "#    hello     \n",
    "print(len(model.wv.get_vector('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_vectorOOV1(s):\n",
    "  try:\n",
    "    return np.array(model.wv.get_vector(s))\n",
    "  except KeyError:\n",
    "    #print(\"$$$$$$$\")\n",
    "    return np.random.random((300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2960765/3032423105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectorOOV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "a = get_vectorOOV1('hello')\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_type = [\"Cross site scripting\",\"SQL injection\",\"Buffer overflow\",\"Directory traversal\",\" Cross-site request forgery\",\" PHP file inclusion\",\" Use-after-free\",\"  Integer overflow\",\" Untrusted search path\",\" Format string\",\" CRLF injection\",\" XML External Entity\",\"other\"]\n",
    "root_cause = [\" Input Validation Error\",\" Boundary Condition Error\",\" Failure to Handle Exceptional Conditions\",\"  Design Error\",\" Access Validation Error\",\"  Atomicity Error\",\" Race Condition Error\",\"  Serialization Error\",\" Configuration Error\",\" Origin Validation Error\",\" Environment Error\"]\n",
    "att_vector = [\" Environment Error\",\" Via some crafted data\",\" By executing the script\",\"  HTTP protocol correlation\",\" Call API\",\"  Others\"]\n",
    "att_type = [\" Remote attacker\",\"  Local attacker\",\" Authenticated user\",\"  Context-dependent\",\" Physically proximate attacker\",\"  Others\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_type_ = []\n",
    "for i in range(len(v_type)):\n",
    "    content = v_type[i]\n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    v_type_.append(content)\n",
    "    #print(content)\n",
    "root_cause_ = []\n",
    "for i in range(len(root_cause)):\n",
    "    content = root_cause[i]\n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    root_cause_.append(content)\n",
    "att_vector_ = []\n",
    "for i in range(len(att_vector)):\n",
    "    content = att_vector[i]\n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    att_vector_.append(content)\n",
    "att_type_ = []\n",
    "for i in range(len(att_type)):\n",
    "    content = att_type[i]\n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    att_type_.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 20 30 12\n",
    "v_type_vec=[]\n",
    "for i in range(len(v_type_)):\n",
    "    temp = []\n",
    "    for j in range(len(v_type_[i])):\n",
    "        temp.append(get_vectorOOV1(v_type_[i][j]))\n",
    "    for j in range(10-len(v_type_[i])):\n",
    "        temp.append(np.random.random((300,)))\n",
    "    v_type_vec.append(temp)\n",
    "root_cause_vec = []\n",
    "for i in range(len(root_cause_)):\n",
    "    temp = []\n",
    "    for j in range(len(root_cause_[i])):\n",
    "        temp.append(get_vectorOOV1(root_cause_[i][j]))\n",
    "    for j in range(20-len(root_cause_[i])):\n",
    "        temp.append(np.random.random((300,)))\n",
    "    root_cause_vec.append(temp)\n",
    "att_vector_vec = []\n",
    "for i in range(len(att_vector_)):\n",
    "    temp = []\n",
    "    for j in range(len(att_vector_[i])):\n",
    "        temp.append(get_vectorOOV1(att_vector_[i][j]))\n",
    "    for j in range(30-len(att_vector_[i])):\n",
    "        temp.append(np.random.random((300,)))\n",
    "    att_vector_vec.append(temp)\n",
    "att_type_vec = []\n",
    "for i in range(len(att_type_)):\n",
    "    temp = []\n",
    "    for j in range(len(att_type_[i])):\n",
    "        temp.append(get_vectorOOV1(att_type_[i][j]))\n",
    "    for j in range(12-len(att_type_[i])):\n",
    "        temp.append(np.random.random((300,)))\n",
    "    att_type_vec.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_type():\n",
    "    pass\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def modify_str(content):\n",
    "    content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "    #       \n",
    "    content = re.sub(\"\\s+\", \" \", content)\n",
    "    #     \n",
    "    content = re.sub(\"\\n\", \" \", content)\n",
    "    #          \n",
    "    content = content.strip()\n",
    "    #print(content)\n",
    "    content = content.split(\" \")\n",
    "    #            \n",
    "    content = [word.lower() for word in content]\n",
    "    \n",
    "    return content\n",
    "\n",
    "def get_vectorOOV1(word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except:\n",
    "        return np.random.random((300,))\n",
    "\n",
    "def unify_len(words, length):\n",
    "    vec = []\n",
    "    if len(words) < length:\n",
    "        for i in range(len(words)):\n",
    "            vec.append(get_vectorOOV1(words[i]))\n",
    "        for i in range(length-len(words)):\n",
    "            vec.append(np.random.random((300,)))\n",
    "    elif len(words) > length:\n",
    "        for i in range(length):\n",
    "            vec.append(get_vectorOOV1(words[i]))\n",
    "    else:\n",
    "        for i in range(length):\n",
    "            vec.append(get_vectorOOV1(words[i]))\n",
    "    return torch.Tensor(np.array(vec))\n",
    "    \n",
    "\n",
    "# def cosine_similarity(vec1, vec2):\n",
    "#     # a= vec1\n",
    "#     # b= vec2\n",
    "#     # cos1 = np.sum(a * b)\n",
    "#     # cos21 = np.sqrt(sum(a ** 2))\n",
    "#     # cos22 = np.sqrt(sum(b ** 2))\n",
    "#     # cosine_value = cos1 / float(cos21 * cos22)\n",
    "    \n",
    "#     return cosine_value\n",
    "\n",
    "def get_class(tensor1, tensor2):\n",
    "    #            \n",
    "    tensor1 = torch.Tensor(np.array(tensor1))\n",
    "    tensor2 = torch.Tensor(np.array(tensor2))\n",
    "    # print(tensor1.shape)\n",
    "    # print(tensor2.shape)\n",
    "    class_ = 0\n",
    "    score = -999999\n",
    "    for i in range(len(tensor2)):\n",
    "        #       \n",
    "        #score1 = nn.PairwiseDistance(tensor1, tensor2[i])\n",
    "        # pdist = nn.PairwiseDistance(p=2)\n",
    "        # score1 = pdist(tensor1,tensor2[i])\n",
    "        \n",
    "\n",
    "#     ，          label    ，                      \n",
    "#     ，                 \n",
    "\n",
    "        #############################################\n",
    "        score1 = F.cosine_similarity(tensor1, tensor2[i], dim=-1)\n",
    "        #print(score1.shape)\n",
    "        temp = 0\n",
    "        #             \n",
    "\n",
    "        for j in range(len(score1)):\n",
    "            temp += score1[j]\n",
    "        #print(i)\n",
    "\n",
    "\n",
    "        \n",
    "        if(temp > score):\n",
    "            score = temp\n",
    "            class_ = i\n",
    "\n",
    "\n",
    "\n",
    "            # print(\"*****\")\n",
    "            # print(class_)\n",
    "            \n",
    "       \n",
    "        #print(score1.shape)\n",
    "        # if(score > score):\n",
    "        #     score = cosine_similarity(tensor1, tensor2[i])\n",
    "        #     class_ = i\n",
    "    return class_\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['remote', 'attacker'], ['local', 'attacker'], ['authenticated', 'user'], ['context', 'dependent'], ['physically', 'proximate', 'attacker'], ['others']]\n",
      "remote attackers\n",
      "2\n",
      "remote attackers\n",
      "2\n",
      "local users\n",
      "3\n",
      "remote attackers\n",
      "2\n",
      "local users\n",
      "0\n",
      "local users\n",
      "1\n",
      "a remote attacker\n",
      "5\n",
      "a local user\n",
      "0\n",
      "a remote attacker\n",
      "1\n",
      "files\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "#               \n",
    "#     vtype\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/data_4aspects.csv'\n",
    "path_v_type = '/home/user/Programs/PMA_lihang/data/data_v_type.csv'\n",
    "path_root_cause = '/home/user/Programs/PMA_lihang/data/data_root_cause.csv'\n",
    "path_att_vector = '/home/user/Programs/PMA_lihang/data/data_att_vector.csv'\n",
    "path_att_type = '/home/user/Programs/PMA_lihang/data/data_att_type.csv'\n",
    "\n",
    "#  data\n",
    "f_data = open(path_data, 'r')\n",
    "lines1 = csv.reader(f_data)\n",
    "\n",
    "pd= []\n",
    "att_type = []\n",
    "impact = []\n",
    "att_vec = []\n",
    "aoot_cause = []\n",
    "v_type = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "print(att_type_)\n",
    "for line in lines1:\n",
    "    #          \n",
    "    # if line[1] != '':\n",
    "    #     pd.append(line[0])\n",
    "    #     pd.append(line[1])\n",
    "    #     pd.append(judge_type(line[1],1))\n",
    "\n",
    "    \n",
    "    if line[2] != '':#12\n",
    "        print(line[2])\n",
    "        att_type.append(line[0])\n",
    "        att_type.append(line[2])\n",
    "        temp = modify_str(line[2])\n",
    "        tensor1 = unify_len(temp, 12)\n",
    "        \n",
    "        class_ = get_class(tensor1, att_type_vec)\n",
    "        print(class_)\n",
    "\n",
    "\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "    continue\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #att_type.append(judge_type(line[2],2))\n",
    "    # if line[3] != '':\n",
    "    #     impact.append(line[0])\n",
    "    #     impact.append(line[3])\n",
    "    #     impact.append(judge_type(line[3],3))\n",
    "    if line[4] != '':\n",
    "        att_vec.append(line[0])\n",
    "        att_vec.append(line[4])\n",
    "        att_vec.append(judge_type(line[4],4))\n",
    "    if line[5] != '':\n",
    "        root_cause.append(line[0])\n",
    "        root_cause.append(line[5])\n",
    "        root_cause.append(judge_type(line[5],5))\n",
    "    if line[6] != '':\n",
    "        v_type.append(line[0])\n",
    "        v_type.append(line[6])\n",
    "        v_type.append(judge_type(line[6],6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# tensor1 = torch.Tensor(np.array(v1_v))\n",
    "# tensor2 = torch.Tensor(np.array(v2_v))\n",
    "\n",
    "# # tensor2 = torch.rand(2,10,100)\n",
    "# # tensor1 = torch.rand(2,10,100)\n",
    "# cos_sim = F.cosine_similarity(tensor1, tensor2, dim=-1)\n",
    "# print(cos_sim.shape)\n",
    "#print(tensor1.shape)\n",
    "#path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.2630, 15.9417, 16.5428, 15.0729, 16.9985, 16.4696, 15.1755, 15.7373,\n",
      "        16.4147, 15.7186, 16.0995, 16.1465, 16.6926, 17.3046, 14.1393, 17.4384,\n",
      "        16.5351, 15.8915, 16.1272, 17.0916, 14.3671, 15.6615, 17.9674, 16.3300,\n",
      "        15.6318, 16.2314, 17.4064, 16.5842, 16.0641, 17.1965, 16.4333, 17.8375,\n",
      "        16.5789, 14.8076, 15.6407, 15.8578, 14.4462, 15.0299, 13.4499, 16.6506,\n",
      "        15.8602, 14.8332, 16.3991, 15.6704, 17.1392, 16.0263, 14.1426, 17.0748,\n",
      "        15.3208, 14.9338, 17.9405, 15.4883, 15.8931, 15.8806, 13.9737, 16.1329,\n",
      "        16.3328, 15.8651, 15.0710, 17.4535, 14.4484, 16.6534, 15.8607, 16.1487,\n",
      "        16.9725, 16.2949, 16.3714, 16.2201, 16.0408, 17.6187, 15.6194, 16.1666,\n",
      "        14.8021, 16.7409, 16.4020, 17.4840, 15.1112, 14.8517, 15.0357, 16.6590,\n",
      "        16.1822, 14.2648, 16.7935, 15.4563, 13.6154, 15.5321, 17.2751, 14.6336,\n",
      "        16.9902, 14.5713, 16.9213, 16.5392, 16.4104, 16.4924, 16.5962, 14.5819,\n",
      "        15.1864, 14.5206, 15.7057, 14.3767])\n"
     ]
    }
   ],
   "source": [
    "pdist = nn.PairwiseDistance(p=2)\n",
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "output = pdist(input1, input2)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
