{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1         \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "\n",
    "# Load word2vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin', binary = True) \n",
    "\n",
    "# Check dimension of word vectors\n",
    "model.vector_size\n",
    "\n",
    "# vec = model['nothing']\n",
    "# print(vec[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6451187335092349\n",
      "total 758\n",
      "right 489\n"
     ]
    }
   ],
   "source": [
    "#  v type       \n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'\n",
    "\n",
    "def right_wrong_work(words):\n",
    "  for i in range(len(words)):\n",
    "    #print(\"*******\",i)\n",
    "    # print(words[i].lower())\n",
    "    # print(\"*\"*50)\n",
    "    if('xss' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      # list_content.append(0)\n",
    "      return True\n",
    "    if( 'cross' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('site' in words[i].lower()):\n",
    "      #print(\" 0\")\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('sql' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'inject' in words[i].lower()):\n",
    "      return True\n",
    "    if('buffer' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'overflow' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'directory' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'traversal' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'request' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'forgery' in words[i].lower() ):\n",
    "      return True\n",
    "    if(  'php' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'inclusion' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'use' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'free' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'integer' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'untrust' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'search' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'format' in words[i].lower()):\n",
    "        return True\n",
    "    if( 'string' in words[i].lower()):\n",
    "        return True\n",
    "    if( 'crlf' in words[i].lower()):\n",
    "        return True\n",
    "    if( 'xml' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'external' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'entity' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'unknow' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'unspecif' in words[i].lower()):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "f = open(path_data,'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "total = 0\n",
    "right = 0\n",
    "jqk = 0\n",
    "for line in lines:\n",
    "  content = line[2]\n",
    "  \n",
    "  words = content.split(\" \")#product embedding\n",
    "  if len(words) < 10:\n",
    "    continue\n",
    "  total += 1\n",
    "  if(right_wrong_work(words)):\n",
    "    right += 1\n",
    "print(right/total)\n",
    "print(\"total\",total)\n",
    "print(\"right\",right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2877094972067039\n",
      "total 1074\n",
      "right 309\n"
     ]
    }
   ],
   "source": [
    "#  root cause       \n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'\n",
    "\n",
    "def right_wrong_work(words):\n",
    "  for i in range(len(words)):\n",
    "    #print(\"*******\",i)\n",
    "    # print(words[i].lower())\n",
    "    # print(\"*\"*50)\n",
    "    if('input' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      # list_content.append(0)\n",
    "      return True\n",
    "    if( 'validat' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('boundary' in words[i].lower()):\n",
    "      #print(\" 0\")\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('fail' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'handle' in words[i].lower()):\n",
    "      return True\n",
    "    if('except' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'condition' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'design' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'access' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'atomicit' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'race' in words[i].lower() ):\n",
    "      return True\n",
    "    if(  'serializ' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'config' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'origin' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'environ' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'deal' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'authen' in words[i].lower()):\n",
    "      return True\n",
    "    # if( 'unknow' in words[i].lower()):\n",
    "    #   return True\n",
    "    # if(  'unspecif' in words[i].lower()):\n",
    "    #   return True\n",
    "  return False\n",
    "\n",
    "f = open(path_data,'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "total = 0\n",
    "right = 0\n",
    "jqk = 0\n",
    "for line in lines:\n",
    "  content = line[3]\n",
    "  \n",
    "  words = content.split(\" \")#product embedding\n",
    "  if len(words) < 10:\n",
    "    continue\n",
    "  total += 1\n",
    "  if(right_wrong_work(words)):\n",
    "    right += 1\n",
    "print(right/total)\n",
    "print(\"total\",total)\n",
    "print(\"right\",right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6717557251908397\n"
     ]
    }
   ],
   "source": [
    "#  attack vector 30       \n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'\n",
    "list_content = []\n",
    "list_file = []\n",
    "path_temp = '/home/user/Programs/PMA_lihang/data/pt1_file2_attvec.csv'\n",
    "\n",
    "specified_length = 11\n",
    "\n",
    "\n",
    "def get_vectorOOV1(s):\n",
    "  try:\n",
    "    return np.array(model.get_vector(s))\n",
    "  except KeyError:\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_aspect_vec1(words):\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        word_vector = get_vectorOOV1(word)\n",
    "        vector.append(word_vector)\n",
    "    # print(len(vector))\n",
    "    tensor = torch.Tensor(np.array(vector))\n",
    "    return tensor    \n",
    "# 6               \n",
    "\n",
    "\n",
    "\n",
    "def deal(words):\n",
    "  if(len(words) < specified_length):\n",
    "    for i in range(specified_length-len(words)):\n",
    "      words.append('nothingitis')  \n",
    "  if(len(words) > specified_length):\n",
    "      words = words[:specified_length]\n",
    "  return words\n",
    "\n",
    "def right_wrong(index,max_score,words):\n",
    "  for i in range(len(words)):\n",
    "    #print(\"*******\",i)\n",
    "    # print(words[i].lower())\n",
    "    # print(\"*\"*50)\n",
    "    if(max_score == 0 and 'field' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      # list_content.append(0)\n",
    "      return True\n",
    "    if(max_score == 0 and 'argumen' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if(max_score == 0 and 'parameter' in words[i].lower()):\n",
    "      #print(\" 0\")\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if(max_score == 1 and 'craft' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 1 and 'manu' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 1 and 'skill' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 2 and 'execut' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 2 and 'run' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 2 and 'script' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 3 and 'http' in words[i].lower() ):\n",
    "      return True\n",
    "    if(max_score == 3 and 'protocol' in words[i].lower() ):\n",
    "      return True\n",
    "    if(max_score == 3 and 'correlat' in words[i].lower() ):\n",
    "      return True\n",
    "    if(max_score == 4 and 'call' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 4 and 'api' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 5 and 'unknow' in words[i].lower()):\n",
    "      return True\n",
    "    if( max_score == 5 and 'unspecif' in words[i].lower()):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def right_wrong_work(words):\n",
    "  for i in range(len(words)):\n",
    "    #print(\"*******\",i)\n",
    "    # print(words[i].lower())\n",
    "    # print(\"*\"*50)\n",
    "    if('field' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      # list_content.append(0)\n",
    "      return True\n",
    "    if( 'argumen' in words[i].lower()):\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('parameter' in words[i].lower()):\n",
    "      #print(\" 0\")\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if('craft' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'manu' in words[i].lower()):\n",
    "      return True\n",
    "    if('skill' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'execut' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'run' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'script' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'http' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'protocol' in words[i].lower() ):\n",
    "      return True\n",
    "    if(  'correlat' in words[i].lower() ):\n",
    "      return True\n",
    "    if( 'call' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'api' in words[i].lower()):\n",
    "      return True\n",
    "    if( 'unknow' in words[i].lower()):\n",
    "      return True\n",
    "    if(  'unspecif' in words[i].lower()):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "f = open(path_data,'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "total = 0\n",
    "right = 0\n",
    "jqk = 0\n",
    "for line in lines:\n",
    "  content = line[5]\n",
    "  \n",
    "  words = content.split(\" \")#product embedding\n",
    "  if len(words) < 3:\n",
    "    continue\n",
    "  total += 1\n",
    "  if(right_wrong_work(words)):\n",
    "    right += 1\n",
    "print(right/total)\n",
    "\n",
    "def dealsym(words):\n",
    "    #                     \n",
    "    #print(words)\n",
    "    words = re.sub(r\"[^A-Za-z0-9]\", \" \", words)\n",
    "    #print(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def judge_acc(aspect_index, attacker_type_embedding):\n",
    "  f = open(path_data,'r',encoding='utf-8')\n",
    "  lines = csv.reader(f)\n",
    "  total = 0\n",
    "  right = 0\n",
    "  jqk = 0\n",
    "  for line in lines:\n",
    "    content = line[aspect_index]\n",
    "    # print(type(content), dealsym(content))\n",
    "    # break\n",
    "    # list_content = []\n",
    "    #print(len(line))\n",
    "    if content == '' :\n",
    "        continue\n",
    "    \n",
    "    # print(type(line[aspect_index]))\n",
    "    # print(\"***\"*20)\n",
    "    content = dealsym(content)\n",
    "    words = content.split(\" \")#product embedding\n",
    "    if len(words) < 3:\n",
    "      continue\n",
    "    # print(words)\n",
    "\n",
    "    total += 1\n",
    "    words = deal(words)\n",
    "    # print(words)\n",
    "    # print(\"*\"*20)\n",
    "    # break\n",
    "    if(len(words) != specified_length):\n",
    "      print(\"length error\")\n",
    "      print(total)\n",
    "      print(len(words))\n",
    "      break\n",
    "    \n",
    "    embedding = get_aspect_vec1(words)\n",
    "    max_score = -999999\n",
    "    max_class = -1\n",
    "    # print(type(attacker_type_embedding))\n",
    "    #     \n",
    "    print(\"=====\"*20)\n",
    "    print(\"  \", words)\n",
    "    for index, i in enumerate(attacker_type_embedding):\n",
    "      # print(embedding.shape)\n",
    "      # print(i.shape)\n",
    "      # print(\"*\"*20)\n",
    "      # break\n",
    "      cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "      output = cos(embedding, i)\n",
    "      sum = 0\n",
    "      for j in output:\n",
    "        sum += j\n",
    "      \n",
    "      # print(\"  \", index)\n",
    "      print(\"  \", index, \"  \", sum)\n",
    "      if(sum > max_score):\n",
    "        max_score = sum\n",
    "        max_class = index\n",
    "    print(\"       \")\n",
    "    print(max_class) \n",
    "    \n",
    "    # list_content.append(max_class)\n",
    "    # list_content.append(line[aspect_index])\n",
    "    # list_file.append(list_content)\n",
    "    if(right_wrong(max_class,words)):\n",
    "      print(\"  \")\n",
    "      right += 1\n",
    "    jqk += 1\n",
    "    if jqk == 2:\n",
    "      break\n",
    "    # print(right)\n",
    "    #print(\"#\"*20)\n",
    "    # break\n",
    "\n",
    "\n",
    "  f = open(path_temp,'a',encoding = 'utf-8',newline='')\n",
    "  for i in list_file:\n",
    "    csv.writer(f).writerow(i)\n",
    "  # print(total)\n",
    "  return right/total  #embedding\n",
    "\n",
    "def get_attacker_type_embedding():\n",
    "  attacker_type = [\"Via field arguments or parameter\",\"Via some crafted data\",\"By executing the script\",\"HTTP protocol correlation\",\"Call API\",\"others\"]\n",
    "  attacker_type_embedding = []\n",
    "  for i in range(len(attacker_type)):\n",
    "    # print(i)\n",
    "    words = attacker_type[i].split(\" \")\n",
    "    # print(words)\n",
    "    words = deal(words)\n",
    "    attacker_type_embedding.append(get_aspect_vec1(words))\n",
    "    #print(attacker_type_embedding[i].shape)\n",
    "  return attacker_type_embedding\n",
    "  # print(words)\n",
    "  #attacker_type[i] = words\n",
    "#attacker_type_embedding = model[attacker_type]\n",
    "# print(attacker_type_embedding[0].shape)\n",
    "#attacker_type_embedding = model[attacker_type]\n",
    "#             \n",
    "\n",
    "#      \n",
    "\n",
    "# attacker_acc = judge_acc(5,get_attacker_type_embedding())\n",
    "# print(attacker_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "##     \n",
    "\n",
    "path_data = \"/home/user/Programs/PMA_lihang/data/data_4aspects.csv\"\n",
    "path_stt_type_label = \"/home/user/Programs/PMA_lihang/data/data_att_type.csv\"\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 300])\n",
      "tensor([ True, False,  True,  True, False])\n",
      "************************************************************\n",
      "   tensor(2.6495)\n",
      "   tensor(2.1215)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "data = ['nothingitis','distloc', 'parameter','nothingitis','nothingitis']\n",
    "data2 = ['nothingitis','Call','API','nothingitis','nothingitis']\n",
    "data3 = ['nothingitis','nothingitis','arguments', 'parameter','nothingitis']\n",
    "class DotProductSimilarity(nn.Module):\n",
    " \n",
    "    def __init__(self, scale_output=False):\n",
    "        super(DotProductSimilarity, self).__init__()\n",
    "        self.scale_output = scale_output\n",
    " \n",
    "    def forward(self, tensor_1, tensor_2):\n",
    "        result = (tensor_1 * tensor_2).sum(dim=-1)\n",
    "        if self.scale_output:\n",
    "            # TODO why allennlp do multiplication at here ?\n",
    "            result /= math.sqrt(tensor_1.size(-1))\n",
    "        return result\n",
    "    \n",
    "list = []\n",
    "list.append(get_aspect_vec1(data))\n",
    "list.append(get_aspect_vec1(data2))\n",
    "list.append(get_aspect_vec1(data3))\n",
    "print(list[0].shape)\n",
    "max_score = -999999\n",
    "max_class = -1\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "\n",
    "# print(list[0].mul(list[1]))\n",
    "# print(list[0].mul(list[2]))\n",
    "\n",
    "\n",
    "func =DotProductSimilarity()\n",
    "print(func(list[0],list[1]) > func(list[0],list[2]))\n",
    "print(\"***\"*20)\n",
    "\n",
    "\n",
    "output = cos(list[0],list[1])\n",
    "sum = 0\n",
    "for j in output:\n",
    "    sum += j\n",
    "# print(\"  \", index)\n",
    "print(\"  \", sum)\n",
    "output = cos(list[0],list[2])\n",
    "sum = 0\n",
    "for j in output:\n",
    "    sum += j\n",
    "# print(\"  \", index)\n",
    "print(\"  \", sum)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 16558, 0, 226, 6680, 9220, 7209, 3322, 4089, 4432, 3544, 2827, 1899, 1551, 2032, 1346, 981, 822, 632, 581, 402, 388, 345, 500, 238, 196, 250, 223, 145, 134, 98, 76, 81, 69, 55, 35, 37, 24, 31, 34, 28, 18, 13, 11, 16, 12, 33, 11, 6, 11, 10, 8, 10, 2, 2, 21, 2, 3, 3, 3, 4, 1, 6, 1, 3, 1, 1, 1, 3, 2, 0, 2, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#        \n",
    "\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'\n",
    "f = open(path_data,'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "\n",
    "length = []\n",
    "for i in range(145):\n",
    "  length.append(0)\n",
    "\n",
    "\n",
    "max = 0\n",
    "for line in lines:\n",
    "    #print(len(line))\n",
    "    attv = line[5].split(\" \")\n",
    "    length[len(attv)] += 1\n",
    "print(length)\n",
    "\n",
    "\n",
    "#30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6880745480454888"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  attacker type 11       \n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path_data = '/home/user/Programs/PMA_lihang/data/pt1_file2.csv'\n",
    "specified_length = 11\n",
    "def get_vectorOOV1(s):\n",
    "  try:\n",
    "    return np.array(model.get_vector(s))\n",
    "  except KeyError:\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_aspect_vec1(words):\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        word_vector = get_vectorOOV1(word)\n",
    "        vector.append(word_vector)\n",
    "    # print(len(vector))\n",
    "    tensor = torch.Tensor(np.array(vector))\n",
    "    return tensor    \n",
    "# 6               \n",
    "\n",
    "\n",
    "\n",
    "def deal(words):\n",
    "  if(len(words) < specified_length):\n",
    "    for i in range(specified_length-len(words)):\n",
    "      words.append('nothingitis')  \n",
    "  if(len(words) > specified_length):\n",
    "      words = words[:specified_length]\n",
    "  return words\n",
    "\n",
    "def right_wrong(max_score,words):\n",
    "  for i in range(len(words)):\n",
    "    # print(words[i].lower())\n",
    "    # print(\"*\"*50)\n",
    "    if(max_score == 0 and words[i].lower() == 'remote'):\n",
    "      #print(\"right\")\n",
    "      return True\n",
    "    if(max_score == 1 and words[i].lower() == 'local'):\n",
    "      return True\n",
    "    if(max_score == 2 and 'authent' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 3 and 'context' in words[i].lower() ):\n",
    "      return True\n",
    "    if(max_score == 4 and 'physical' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 5 and 'unknow' in words[i].lower()):\n",
    "      return True\n",
    "    if(max_score == 5 and 'unspecif' in words[i].lower()):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "  \n",
    "\n",
    "def judge_acc(aspect_index, attacker_type_embedding):\n",
    "  f = open(path_data,'r',encoding='utf-8')\n",
    "  lines = csv.reader(f)\n",
    "  total = 0\n",
    "  right = 0\n",
    "\n",
    "  for line in lines:\n",
    "    \n",
    "    #print(len(line))\n",
    "    if line[aspect_index] == '':\n",
    "        continue\n",
    "    total += 1\n",
    "    words = line[aspect_index].split(\" \")#product embedding\n",
    "    # print(words)\n",
    "    words = deal(words)\n",
    "    # print(words)\n",
    "    # print(\"*\"*20)\n",
    "    # break\n",
    "    if(len(words) != specified_length):\n",
    "      print(\"length error\")\n",
    "      print(total)\n",
    "      print(len(words))\n",
    "      break\n",
    "    embedding = get_aspect_vec1(words)\n",
    "    max_score = -999999\n",
    "    max_class = -1\n",
    "    # print(type(attacker_type_embedding))\n",
    "    #     \n",
    "    for index, i in enumerate(attacker_type_embedding):\n",
    "      # print(embedding.shape)\n",
    "      # print(i.shape)\n",
    "      # print(\"*\"*20)\n",
    "      # break\n",
    "      cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "      output = cos(embedding, i)\n",
    "      sum = 0\n",
    "      for j in output:\n",
    "        sum += j\n",
    "        \n",
    "      if(sum > max_score):\n",
    "        max_score = sum\n",
    "        max_class = index\n",
    "    # print(\"       \")\n",
    "    # print(max_class) \n",
    "    \n",
    "    if(right_wrong(max_class,words)):\n",
    "      right += 1\n",
    "\n",
    "    # print(right)\n",
    "    #print(\"#\"*20)\n",
    "    # break\n",
    "\n",
    "\n",
    "\n",
    "  # print(total)\n",
    "  return right/total  #embedding\n",
    "\n",
    "def get_attacker_type_embedding():\n",
    "  attacker_type = [\"remote attacker\",\"local attacker\",\"authenticated user\",\"context dependent\",\"physically proximate attacker\",\"others\"]\n",
    "  attacker_type_embedding = []\n",
    "  for i in range(len(attacker_type)):\n",
    "    # print(i)\n",
    "    words = attacker_type[i].split(\" \")\n",
    "    # print(words)\n",
    "    words = deal(words)\n",
    "    attacker_type_embedding.append(get_aspect_vec1(words))\n",
    "  return attacker_type_embedding\n",
    "  # print(words)\n",
    "  #attacker_type[i] = words\n",
    "#attacker_type_embedding = model[attacker_type]\n",
    "# print(attacker_type_embedding[0].shape)\n",
    "#attacker_type_embedding = model[attacker_type]\n",
    "#             \n",
    "\n",
    "\n",
    "\n",
    "attacker_acc = judge_acc(2,get_attacker_type_embedding())\n",
    "attacker_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_data,'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "i = 0\n",
    "max_len = []\n",
    "for i in range(67):\n",
    "    max_len.append(0)\n",
    "for line in lines:\n",
    "    \n",
    "    words = line[2].split(\" \")\n",
    "    max_len[len(words)] += 1\n",
    "\n",
    "max_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'nothingitis' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1324419/3913499527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nothingitis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'nothingitis' not present\""
     ]
    }
   ],
   "source": [
    "model.get_vector('via shell metacharacters in the distloc parameter.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0   dt.append(head)\n",
    "1   dt.append(pd)#product     \n",
    "2   dt.append(aat)#attacker type    \n",
    "3   dt.append(rootc)#rootcause   \n",
    "4   dt.append(im)#impact\n",
    "5   dt.append(at)#attacker vector\n",
    "   0-5       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
