{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 14, 45, 67, 74, 75, 105, 112, 113, 152, 159, 160, 196, 208, 210, 215, 231, 292, 311, 342, 435, 442, 448, 455, 457, 467, 475, 483, 502, 567, 589, 643, 664, 667, 674, 695, 717, 739, 749, 799, 805, 807, 851, 926, 930, 931, 938, 947, 962, 975, 1001, 1026, 1166, 1224, 1302, 1303, 1317, 1318, 1380, 1390, 1403, 1708, 1788, 1808, 1853, 1877, 1878, 1879, 1880, 1894, 1910, 1937, 1944, 1969, 1995, 1996, 1997, 2038, 2070, 2083, 2146, 2159, 2163, 2164, 2166, 2184, 2197, 2206, 2213, 2217, 2218, 2220, 2222, 2223, 2224, 2227, 2232, 2239, 2241]\n"
     ]
    }
   ],
   "source": [
    "path = '/home/user/Programs/PMA_lihang/open_part/cveSplit.csv'\n",
    "#     \n",
    "f = open(path, 'r',encoding='utf-8')\n",
    "#     \n",
    "lines = f.readlines()\n",
    "i = 0\n",
    "j = 0\n",
    "data = []\n",
    "for line in lines:\n",
    "    i += 1\n",
    "    if ',,' in line:\n",
    "        continue\n",
    "    else:\n",
    "        data.append(i)\n",
    "        j+=1\n",
    "    if(j==100):\n",
    "        break\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An elevation of privilege vulnerability\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "str1 = \"An elevation of privilege vulnerability in the in MediaTek I2C driver could enable a local malicious application to execute arbitrary code within the context of the kernel. This issue is rated as High because it first requires compromising a privileged process. Product: Android. Versions: N/A. Android ID: A-31224428. References: MT-ALPS02943467.\"\n",
    "#        ，       'in'     \n",
    "befor_in = re.findall('(.*?) in ',str1)[0]\n",
    "print(befor_in)\n",
    "pass\n",
    "\n",
    "# str2 = \"A denial of service vulnerability in the NVIDIA camera driver could enable an attacker to cause a local permanent denial of service\"\n",
    "# sub1 = re.findall(' could enable a(n)? .*? to (.*?)(?:(?:, )|(?:\\. )|(?:\\.\\Z))',str1)\n",
    "# sub2 = re.findall(' could enable a(n)? .*? to (.*?)(?:(?:, )|(?:\\. )|(?:\\Z))',str2)\n",
    "# print(len(sub1),sub1)\n",
    "# print(len(sub2),sub2)\n",
    "\n",
    "# a = re.findall('a(n)? (.*)',str1)\n",
    "# print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jack\n",
      "1 like\n",
      "2 dog\n",
      "3 jack\n",
      "4 like\n",
      "5 cat\n",
      "6 animal\n"
     ]
    }
   ],
   "source": [
    "vocab = ['jack', 'like', 'dog', 'jack', 'like', 'cat', 'animal']\n",
    "# vocab = list(set(vocab))\n",
    "for i,w in enumerate(vocab):\n",
    "    print(i, w)\n",
    "# print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAchad', 'adcadc']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#2         \n",
    "lista = ['AAchad','adcadc']\n",
    "print(lista)\n",
    "print('aa' in lista[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "True and i > 1 or i == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-filrjVy7ECPQGvKpcjxTRHID/user-Rik5RoCZmUjAQkx64cQq4H6d/img-lKmm5K8YRLqwGKBEKLcUnbeX.png?st=2023-02-22T06%3A39%3A57Z&se=2023-02-22T08%3A39%3A57Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-21T21%3A52%3A40Z&ske=2023-02-22T21%3A52%3A40Z&sks=b&skv=2021-08-06&sig=qayYDEtBwnDzwYacC84XBHBpDjd%2By37vywJUlWV8sO4%3D\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-filrjVy7ECPQGvKpcjxTRHID/user-Rik5RoCZmUjAQkx64cQq4H6d/img-OR9vaJvw6H3tyDWU6SOLhYdA.png?st=2023-02-22T06%3A27%3A41Z&se=2023-02-22T08%3A27%3A41Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-21T21%3A52%3A42Z&ske=2023-02-22T21%3A52%3A42Z&sks=b&skv=2021-08-06&sig=fgivn8Y6vqwJJSD4GUd/r0aENMoynRnsn61w7kVyv4A%3D\n"
     ]
    }
   ],
   "source": [
    "#    url\n",
    "https://oaidalleapiprodscus.blob.core.windows.net/private/org-filrjVy7ECPQGvKpcjxTRHID/user-Rik5RoCZmUjAQkx64cQq4H6d/img-OR9vaJvw6H3tyDWU6SOLhYdA.png?st=2023-02-22T06%3A27%3A41Z&se=2023-02-22T08%3A27%3A41Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-21T21%3A52%3A42Z&ske=2023-02-22T21%3A52%3A42Z&sks=b&skv=2021-08-06&sig=fgivn8Y6vqwJJSD4GUd/r0aENMoynRnsn61w7kVyv4A%3D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "path_att_type_label = \"/home/user/Programs/PMA_lihang/data/data_att_type_.csv\"\n",
    "path_v_type_label =\"/home/user/Programs/PMA_lihang/data/data_v_type_.csv\"\n",
    "import csv\n",
    "\n",
    "#     \n",
    "f = open(path_att_type_label, 'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "f1 = open(path_v_type_label, 'r',encoding='utf-8')\n",
    "lines1 = csv.reader(f1)\n",
    "count = []\n",
    "for i in range(10000):\n",
    "    count.append(0)\n",
    "for line in lines:\n",
    "    count[len(line[1].split(\" \"))] += 1\n",
    "print(count)\n",
    "temp =[0, 2, 6, 30, 56, 108, 223, 413, 526, 834, 3055, 1662, 2058, 3723, 3556, 4021, 3340, 3625, 3313, 3283, 3192, 2932, 3160, 2489, 2385, 1982, 1681, 1466, 1327, 1129, 1004, 961, 743, 643, 560, 586, 479, 483, 413, 355, 329, 262, 246, 272, 220, 178, 254, 220, 208, 195, 177, 156, 220, 149, 108, 108, 110, 118, 354, 181, 83, 156, 143]\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "## 1           \n",
    "myPath = '/home/user/lh/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin' #         \n",
    "model = Word2Vec.load(myPath) #      \n",
    "\n",
    "#           -     \n",
    "word2vec = {}\n",
    "i = 0\n",
    "#      \n",
    "# print(len(model.wv.key_to_index))\n",
    "#         padding\n",
    "word2vec['<PAD>'] = torch.zeros(300)\n",
    "for word in model.wv.key_to_index:\n",
    "    # i +=1\n",
    "    # print(word)\n",
    "    # if i == 10:\n",
    "    #     break\n",
    "    word2vec[word] = model.wv.get_vector(word)\n",
    "# print(word2vec['vulnerability'])\n",
    "# print(len(word2vec))\n",
    "# #       key\n",
    "# print(word2vec['<PAD>'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#    \n",
    "path1 = \"/home/user/Programs/PMA_lihang/code_lihang/get_4aspects/data/row_test.csv\"\n",
    "path2 = \"/home/user/Programs/PMA_lihang/data/allitems_v5.csv\"\n",
    "import csv\n",
    "\n",
    "#  path2\n",
    "f = open(path2, 'r',encoding='utf-8')\n",
    "lines = csv.reader(f)\n",
    "kk = 0\n",
    "for line in lines:\n",
    "    data = ''\n",
    "    data = line[0].split(\"\\t##=divide=##\\t\")[1] + \"\\n\"\n",
    "    # print(data)\n",
    "    \n",
    "    #  path1\n",
    "    f1 = open(path1, 'a',encoding='utf-8')\n",
    "    f1.write(data)\n",
    "    # kk += 1\n",
    "    # if kk == 10:\n",
    "    #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model (you can download it from https://code.google.com/archive/p/word2vec/)\n",
    "# In this example, I'm using the pre-trained Google News Word2Vec model.\n",
    "model_path = '/home/user/lh/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin'  # Update the path to the Word2Vec model file\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def calculate_similarity(word1, word2):\n",
    "    try:\n",
    "        similarity = word2vec_model.similarity(word1, word2)\n",
    "        return similarity\n",
    "    except KeyError as e:\n",
    "        print(f\"One or both words not in vocabulary: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'command' and 'field arguments parameter': 0.2818134967237711\n",
      "Similarity between 'command' and 'crafted data': 0.10454437136650085\n",
      "Similarity between 'command' and 'execute script': 0.4082951098680496\n",
      "Similarity between 'command' and 'HTTP protocol correlation': 0.4586053714156151\n",
      "Similarity between 'command' and 'Call API': 0.12137078493833542\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "s1 = \"string\"\n",
    "s1 = \"command\"\n",
    "\n",
    "l1 = \"field arguments parameter\"\n",
    "l1 = \"(computer science) a set of one or more adjacent characters comprising a unit of information (computer science) a reference or value that is passed to a function, procedure, subroutine, command, or program\"\n",
    "l2 = \"crafted data\"\n",
    "\n",
    "l3 = \"execute script\"\n",
    "l4 = \"HTTP protocol correlation\"\n",
    "l5 = \"Call API\"\n",
    "\n",
    "\n",
    "word1 = 'name'\n",
    "word2 = 'field'\n",
    "word2 = 'argument'\n",
    "# word2 = 'parameter'\n",
    "\n",
    "\n",
    "for ln in [l1,l2,l3,l4,l5]:\n",
    "    similarity_score = 0\n",
    "    len = 0\n",
    "    for word2 in ln.split(\" \"):\n",
    "        len += 1\n",
    "        for word1 in s1.split(\" \"):\n",
    "            # print(word1,word2)\n",
    "            similarity_score += calculate_similarity(word1, word2)\n",
    "    print(f\"Similarity between '{s1}' and '{ln}': {similarity_score/len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m    “base”      ipykernel 。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def path_similarity_score(word1, word2):\n",
    "    #   word1   \n",
    "    synsets_word1 = lesk(word1, word1)\n",
    "    \n",
    "    #   word2   \n",
    "    synsets_word2 = lesk(word2, word2)\n",
    "    \n",
    "    #          \n",
    "    if synsets_word1 and synsets_word2:\n",
    "        path_similarity = synsets_word1.path_similarity(synsets_word2)\n",
    "        return path_similarity\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#   \n",
    "word1 = \"vectors\"\n",
    "word2 = \"data\"\n",
    "\n",
    "\n",
    "s1 = \"chained initialization vectors\"\n",
    "\n",
    "l1 = \"field arguments parameter\"\n",
    "l2 = \"crafted data\"\n",
    "l3 = \"execute script\"\n",
    "l4 = \"HTTP protocol correlation\"\n",
    "l5 = \"Call API\"\n",
    "\n",
    "\n",
    "word1 = 'name'\n",
    "word2 = 'field'\n",
    "word2 = 'argument'\n",
    "# word2 = 'parameter'\n",
    "\n",
    "\n",
    "for ln in [l1,l2,l3,l4,l5]:\n",
    "    similarity_score = 0\n",
    "    for word2 in ln.split(\" \"):\n",
    "        for word1 in s1.split(\" \"):\n",
    "            # print(word1,word2)\n",
    "            similarity_score += path_similarity_score(word1, word2)\n",
    "    print(f\"Similarity between '{s1}' and '{ln}': {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5707963267948966\n",
      "1.4711276743037345\n",
      "1.3734007669450157\n",
      "1.2793395323170293\n",
      "1.1902899496825317\n",
      "1.1071487177940904\n",
      "1.0303768265243123\n",
      "0.9600703624056879\n",
      "0.8960553845713438\n",
      "0.83798122500839\n",
      "0.7853981633974483\n",
      "0.7378150601204648\n",
      "0.694738276196703\n",
      "0.6556956262415361\n",
      "0.6202494859828214\n",
      "0.5880026035475675\n",
      "0.5585993153435624\n",
      "0.5317240672588055\n",
      "0.5070985043923368\n",
      "0.48447792903702314\n",
      "0.46364760900080615\n",
      "0.44441920990109884\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for index in range(22):\n",
    "    print(math.pi/2 - math.atan((1e-1)*(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path similarity score between 'vectors' and 'data': 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def path_similarity_score(word1, word2):\n",
    "    #   word1   \n",
    "    synsets_word1 = lesk(word1, word1)\n",
    "    \n",
    "    #   word2   \n",
    "    synsets_word2 = lesk(word2, word2)\n",
    "    \n",
    "    #          \n",
    "    if synsets_word1 and synsets_word2:\n",
    "        path_similarity = synsets_word1.path_similarity(synsets_word2)\n",
    "        return path_similarity\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#   \n",
    "word1 = \"vectors\"\n",
    "word2 = \"data\"\n",
    "\n",
    "score = path_similarity_score(word1, word2)\n",
    "\n",
    "if score is not None:\n",
    "    print(f\"Path similarity score between '{word1}' and '{word2}': {score}\")\n",
    "else:\n",
    "    print(f\"Unable to calculate path similarity for '{word1}' and '{word2}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
